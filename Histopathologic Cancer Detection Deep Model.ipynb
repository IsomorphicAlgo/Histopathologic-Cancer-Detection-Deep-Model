{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histopathologic Cancer Detection Deep Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Utilizing deep learning to detect cancer in histopathologic images.</h3><br>\n",
    "    <h5>By: Michael Hansen</h5>\n",
    "    <h5>Date: 02/04/2025</h5>\n",
    "    <h5>Course: DTSA5511 Deep Learning</h5>\n",
    "    <h5>Instructor: Dr. Ying Sun</h5>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an exploration into Deep Learning and it's use to detect cancer in histopathologic images. The dataset provided in this Kaggle competition is a collection of histopathologic images of breast cancer tumors. The goal is to build a model that can accurately detect cancer in these images.\n",
    "\n",
    "The importance of being able to perform this task with a deep learning model cannot be understated as the benifits are too numerous to count. Thankfully there is a host of brilliant minds that have made a suite of tools that make this task easier. For this project I will be using the [Keras](https://keras.io/) library to build my model, and using ideas such as pooling, dropout, and batch normalization to improve the performance of my model.\n",
    "\n",
    "The dataset can be found [here](https://www.kaggle.com/competitions/histopathologic-cancer-detection/data), and was originally obtained from the [TCGA](https://portal.gdc.cancer.gov/projects/TCGA-BRCA) project. The group at kaggle removed duplicates that originated from probablistic sampling. There are roughly 220,000 labeled images for training and about 57,000 for the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "from plotly.subplots import make_subplots # plotting\n",
    "import plotly.graph_objs as go # plotting\n",
    "\n",
    "from glob import glob # file handling\n",
    "import os # file handling\n",
    "import cv2 # image processing\n",
    "\n",
    "# deep learning\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.applications.vgg19 import VGG19,preprocess_input\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Flatten,Dropout,Concatenate,GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard,CSVLogger,ReduceLROnPlateau,LearningRateScheduler\n",
    "\n",
    "# analysis, data preprocessing, and model evaluation\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G:/My Drive/Academia/C Data/train/5b0212d09ace...</td>\n",
       "      <td>5b0212d09aceaf5f0021bbc6cca1b1b02c0d50cc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G:/My Drive/Academia/C Data/train/5a8d75753adb...</td>\n",
       "      <td>5a8d75753adbc744c45c12744fa29b2353ea2d84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G:/My Drive/Academia/C Data/train/5b1ec76bb7cf...</td>\n",
       "      <td>5b1ec76bb7cfec19dbb807f8114889f775ca032d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G:/My Drive/Academia/C Data/train/5ab099f72ede...</td>\n",
       "      <td>5ab099f72edec5827282e8538ff000b1bc891cbe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G:/My Drive/Academia/C Data/train/5b00ba6ce53a...</td>\n",
       "      <td>5b00ba6ce53ae46bc844fa0c4663a305955494f1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  G:/My Drive/Academia/C Data/train/5b0212d09ace...   \n",
       "1  G:/My Drive/Academia/C Data/train/5a8d75753adb...   \n",
       "2  G:/My Drive/Academia/C Data/train/5b1ec76bb7cf...   \n",
       "3  G:/My Drive/Academia/C Data/train/5ab099f72ede...   \n",
       "4  G:/My Drive/Academia/C Data/train/5b00ba6ce53a...   \n",
       "\n",
       "                                         id  label  \n",
       "0  5b0212d09aceaf5f0021bbc6cca1b1b02c0d50cc      0  \n",
       "1  5a8d75753adbc744c45c12744fa29b2353ea2d84      1  \n",
       "2  5b1ec76bb7cfec19dbb807f8114889f775ca032d      1  \n",
       "3  5ab099f72edec5827282e8538ff000b1bc891cbe      0  \n",
       "4  5b00ba6ce53ae46bc844fa0c4663a305955494f1      1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'G:/My Drive/Academia/C Data/'\n",
    "train_path = file_path + \"train/\"\n",
    "test_path = file_path + \"test/\"\n",
    "train_data_frame = pd.DataFrame({'path': [p.replace('\\\\', '/') for p in glob(os.path.join(train_path,'*.tif'))]})\n",
    "test_data_frame = pd.DataFrame({'path': [p.replace('\\\\', '/') for p in glob(os.path.join(test_path,'*.tif'))]})\n",
    "final_data_frame = pd.concat([train_data_frame, test_data_frame]).reset_index(drop = True)\n",
    "final_data_frame['id'] = final_data_frame.path.map(lambda x: x.split('/')[5].split(\".\")[0])\n",
    "labels = pd.read_csv(file_path + \"train_labels.csv\")\n",
    "df_data = final_data_frame.merge(labels, on = \"id\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(N,data_frame):\n",
    "    \"\"\" This functions loads N images using the data df\n",
    "    \"\"\"\n",
    "    # allocate a numpy array for the images (N, 96x96px, 3 channels, values 0 - 255)\n",
    "    X = np.zeros([N,96,96,3],dtype=np.uint8) \n",
    "    #convert the labels to a numpy array too\n",
    "    y = np.squeeze(data_frame[['label']].to_numpy())[0:N]\n",
    "    #read images one by one, tdqm notebook displays a progress bar\n",
    "    for i, row in data_frame.iterrows():\n",
    "        if i == N:\n",
    "            break\n",
    "        X[i] = cv2.imread(row['path'])\n",
    "          \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lod the pictures\n",
    "N_pics=10000\n",
    "X,y = load_data(N=N_pics,data_frame=df_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in displayed samples:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 488 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Print distribution of labels in our sample\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLabel distribution in displayed samples:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m unique, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_indices\u001b[49m\u001b[43m]\u001b[49m, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(unique, counts):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 488 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Display a few sample images with their labels\n",
    "plt.figure(figsize=(15,6))\n",
    "num_samples = 8\n",
    "\n",
    "# Randomly select indices to display\n",
    "sample_indices = np.random.randint(0, len(X), num_samples)\n",
    "\n",
    "for plotNr, idx in enumerate(sample_indices):\n",
    "    ax = plt.subplot(2, num_samples//2, plotNr+1)\n",
    "    plt.imshow(X[idx])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print distribution of labels in our sample\n",
    "print(f\"\\nLabel distribution in displayed samples:\")\n",
    "unique, counts = np.unique(y[sample_indices], return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"Label {label}: {count} images\")\n",
    "\n",
    "print(f\"\\nTotal dataset label distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"Label {label}: {count} images ({count/len(y)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lay out the size and shape of the test and train\n",
    "\n",
    "# Quick check for duplicates and NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the Data info & cleaning\n",
    "\n",
    "Now we want to see if there is any distiguishable features we can look for visually ourselves. \n",
    "According to \n",
    "https://www.ncbi.nlm.nih.gov/books/NBK9553/#:~:text=Morphologically%2C%20the%20cancerous%20cell%20is,prominent%2C%20the%20cytoplasm%20is%20scarce\n",
    "you can see differences in the size, shape, and potentially color of the cells in the images. This is good reason to dive into attributes of the pixels deeper. Each pixel has 3 channels, red, green, and blue. We can see that these are set on a scale from 0 to 255, 0 being the darkest of the color. \n",
    "\n",
    "And right there we have two characteristics. Brightness and Color. Both of these match characteristics listed in the article above so lets check it out. \n",
    "\n",
    "Comparing pairwise images would be impossible with data of this size and the resources / time at hand. What we can do is compare the amounts of each channel in each! The total sum gives the colors and the average will then give the brightness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Establish a new dataframe that puts an entry for each image, with the amount of R, G, and B in each image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution statistic\n",
    "fig = plt.figure(figsize=(4, 2),dpi=150)\n",
    "plt.bar([1,0], [(y==0).sum(), (y==1).sum()]); #plot a bar chart of the label frequency\n",
    "plt.xticks([1,0],[\"Negative (N={})\".format((y==0).sum()),\"Positive (N={})\".format((y==1).sum())]);\n",
    "plt.ylabel(\"# of samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "positive_samples = X[y == 1]\n",
    "negative_samples = X[y == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_bins = 256 #each possible pixel value will get a bin in the following histograms\n",
    "fig,axs = plt.subplots(4,2,sharey=True,figsize=(8,8),dpi=150)\n",
    "\n",
    "#RGB channels\n",
    "axs[0,0].hist(positive_samples[:,:,:,0].flatten(),bins=nr_of_bins,density=True)\n",
    "axs[0,1].hist(negative_samples[:,:,:,0].flatten(),bins=nr_of_bins,density=True)\n",
    "axs[1,0].hist(positive_samples[:,:,:,1].flatten(),bins=nr_of_bins,density=True)\n",
    "axs[1,1].hist(negative_samples[:,:,:,1].flatten(),bins=nr_of_bins,density=True)\n",
    "axs[2,0].hist(positive_samples[:,:,:,2].flatten(),bins=nr_of_bins,density=True)\n",
    "axs[2,1].hist(negative_samples[:,:,:,2].flatten(),bins=nr_of_bins,density=True)\n",
    "\n",
    "#All channels\n",
    "axs[3,0].hist(positive_samples.flatten(),bins=nr_of_bins,density=True)\n",
    "axs[3,1].hist(negative_samples.flatten(),bins=nr_of_bins,density=True)\n",
    "\n",
    "#Set image labels\n",
    "axs[0,0].set_title(\"Positive samples (N =\" + str(positive_samples.shape[0]) + \")\");\n",
    "axs[0,1].set_title(\"Negative samples (N =\" + str(negative_samples.shape[0]) + \")\");\n",
    "axs[0,1].set_ylabel(\"Red\",rotation='horizontal',labelpad=35,fontsize=12)\n",
    "axs[1,1].set_ylabel(\"Green\",rotation='horizontal',labelpad=35,fontsize=12)\n",
    "axs[2,1].set_ylabel(\"Blue\",rotation='horizontal',labelpad=35,fontsize=12)\n",
    "axs[3,1].set_ylabel(\"RGB\",rotation='horizontal',labelpad=35,fontsize=12)\n",
    "for i in range(4):\n",
    "    axs[i,0].set_ylabel(\"Relative frequency\")\n",
    "axs[3,0].set_xlabel(\"Pixel value\")\n",
    "axs[3,1].set_xlabel(\"Pixel value\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df[\"path\"].size # get the number of images in the training data set\n",
    "X,y = load_data(N=N,df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_samples = None\n",
    "negative_samples = None\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_portion = 0.8 # Specify training/validation ratio\n",
    "split_idx = int(np.round(training_portion * y.shape[0])) #Compute split idx\n",
    "\n",
    "np.random.seed(42) #set the seed to ensure reproducibility\n",
    "\n",
    "#shuffle\n",
    "idx = np.arange(y.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just some network parameters, see above link regarding the layers for details\n",
    "kernel_size = (3,3)\n",
    "pool_size= (2,2)\n",
    "first_filters = 32\n",
    "second_filters = 64\n",
    "third_filters = 128\n",
    "\n",
    "#dropout is used for regularization here with a probability of 0.3 for conv layers, 0.5 for the dense layer at the end\n",
    "dropout_conv = 0.3\n",
    "dropout_dense = 0.5\n",
    "\n",
    "#initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "#now add layers to it\n",
    "\n",
    "#conv block 1\n",
    "model.add(Conv2D(first_filters, kernel_size, input_shape = (96, 96, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(first_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size)) \n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "#conv block 2\n",
    "model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "#conv block 3\n",
    "model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "#a fully connected (also called dense) layer at the end\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(dropout_dense))\n",
    "\n",
    "#finally convert to values of 0 to 1 using the sigmoid activation function\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(0.001), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normally you would want to reshuffle the data between epochs, we don't as we split in-place into training/validation\n",
    "epochs = 3 #how many epochs we want to perform\n",
    "for epoch in range(epochs):\n",
    "    #compute how many batches we'll need\n",
    "    iterations = np.floor(split_idx / batch_size).astype(int) #the floor makes us discard a few samples here, I got lazy...\n",
    "    loss,acc = 0,0 #we will compute running loss and accuracy\n",
    "    with trange(iterations) as t: #display a progress bar\n",
    "        for i in t:\n",
    "            start_idx = i * batch_size #starting index of the current batch\n",
    "            x_batch = X[start_idx:start_idx+batch_size] #the current batch\n",
    "            y_batch = y[start_idx:start_idx+batch_size] #the labels for the current batch\n",
    "\n",
    "            metrics = model.train_on_batch(x_batch, y_batch) #train the model on a batch\n",
    "\n",
    "            loss = loss + metrics[0] #compute running loss\n",
    "            acc = acc + metrics[1] #compute running accuracy\n",
    "            t.set_description('Running training epoch ' + str(epoch)) #set progressbar title\n",
    "            t.set_postfix(loss=\"%.2f\" % round(loss / (i+1),2),acc=\"%.2f\" % round(acc / (i+1),2)) #display metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute how many batches we'll need\n",
    "iterations = np.floor((y.shape[0]-split_idx) / batch_size).astype(int) #as above, not perfect\n",
    "loss,acc = 0,0 #we will compute running loss and accuracy\n",
    "with trange(iterations) as t: #display a progress bar\n",
    "    for i in t:\n",
    "        start_idx = i * batch_size #starting index of the current batch\n",
    "        x_batch = X[start_idx:start_idx+batch_size] #the current batch\n",
    "        y_batch = y[start_idx:start_idx+batch_size] #the labels for the current batch\n",
    "        \n",
    "        metrics = model.test_on_batch(x_batch, y_batch) #compute metric results for this batch using the model\n",
    "        \n",
    "        loss = loss + metrics[0] #compute running loss\n",
    "        acc = acc + metrics[1] #compute running accuracy\n",
    "        t.set_description('Running training') #set progressbar title\n",
    "        t.set_description('Running validation')\n",
    "        t.set_postfix(loss=\"%.2f\" % round(loss / (i+1),2),acc=\"%.2f\" % round(acc / (i+1),2))\n",
    "        \n",
    "print(\"Validation loss:\",loss / iterations)\n",
    "print(\"Validation accuracy:\",acc / iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test_dir = path + 'test/' #specify test data folder\n",
    "test_files = glob(os.path.join(base_test_dir,'*.tif')) #find the test file names\n",
    "submission = pd.DataFrame() #create a dataframe to hold results\n",
    "file_batch = 5000 #we will predict 5000 images at a time\n",
    "max_idx = len(test_files) #last index to use\n",
    "for idx in range(0, max_idx, file_batch): #iterate over test image batches\n",
    "    print(\"Indexes: %i - %i\"%(idx, idx+file_batch))\n",
    "    test_df = pd.DataFrame({'path': test_files[idx:idx+file_batch]}) #add the filenames to the dataframe\n",
    "    test_df['id'] = test_df.path.map(lambda x: x.split('/')[3].split(\".\")[0]) #add the ids to the dataframe\n",
    "    test_df['image'] = test_df['path'].map(cv2.imread) #read the batch\n",
    "    K_test = np.stack(test_df[\"image\"].values) #convert to numpy array\n",
    "    predictions = model.predict(K_test,verbose = 1) #predict the labels for the test data\n",
    "    test_df['label'] = predictions #store them in the dataframe\n",
    "    submission = pd.concat([submission, test_df[[\"id\", \"label\"]]])\n",
    "submission.head() #display first lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index = False, header = True) #create the submission file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
